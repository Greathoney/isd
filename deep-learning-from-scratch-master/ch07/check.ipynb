{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0bd981a0fb0a6bb4c4a7911b53bdee35f4408d0a25f73998a3e0a20a22928721e",
   "display_name": "Python 3.8.8 64-bit ('isd_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_layer_net.py \n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# modified from train_neuralnet.py\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "\n",
    "def train_neuralnet_mnist(x_train, t_train, x_test, t_test, \n",
    "                          input_size=64, hidden_size=10, output_size=10, \n",
    "                          iters_num = 1000, batch_size = 10, learning_rate = 0.1,\n",
    "                          verbose=True):\n",
    "    \n",
    "    network = TwoLayerNet(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Train Parameters\n",
    "    train_size = x_train.shape[0]\n",
    "    iter_per_epoch = int(max(train_size / batch_size, 1))\n",
    "\n",
    "    train_loss_list, train_acc_list, test_acc_list = [], [], []\n",
    "\n",
    "    for step in range(1, iters_num+1):\n",
    "        # get mini-batch\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "\n",
    "        # 기울기 계산\n",
    "        #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "        grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(압도적으로 빠르다)\n",
    "\n",
    "        # Update\n",
    "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "        # loss\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        if verbose and step % iter_per_epoch == 0:\n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            test_acc = network.accuracy(x_test, t_test)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            print('Step: {:4d}\\tTrain acc: {:.5f}\\tTest acc: {:.5f}'.format(step, \n",
    "                                                                            train_acc,\n",
    "                                                                            test_acc))\n",
    "    tracc, teacc = network.accuracy(x_train, t_train), network.accuracy(x_test, t_test)\n",
    "    if verbose:\n",
    "        print('Optimization finished!')\n",
    "        print('Training accuracy: %.2f' % tracc)\n",
    "        print('Test accuracy: %.2f' % teacc)\n",
    "    return tracc, teacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step:  107\tTrain acc: 0.48285\tTest acc: 0.00000\n",
      "Step:  214\tTrain acc: 0.91381\tTest acc: 0.00557\n",
      "Step:  321\tTrain acc: 0.95922\tTest acc: 0.00557\n",
      "Step:  428\tTrain acc: 0.97034\tTest acc: 0.00557\n",
      "Step:  535\tTrain acc: 0.97220\tTest acc: 0.00557\n",
      "Step:  642\tTrain acc: 0.98054\tTest acc: 0.00557\n",
      "Step:  749\tTrain acc: 0.98703\tTest acc: 0.00557\n",
      "Step:  856\tTrain acc: 0.98703\tTest acc: 0.00557\n",
      "Step:  963\tTrain acc: 0.98981\tTest acc: 0.00557\n",
      "Step: 1070\tTrain acc: 0.98795\tTest acc: 0.00557\n",
      "Step: 1177\tTrain acc: 0.99166\tTest acc: 0.00557\n",
      "Step: 1284\tTrain acc: 0.99073\tTest acc: 0.00557\n",
      "Step: 1391\tTrain acc: 0.99537\tTest acc: 0.00557\n",
      "Step: 1498\tTrain acc: 0.99444\tTest acc: 0.00557\n",
      "Step: 1605\tTrain acc: 0.99629\tTest acc: 0.00557\n",
      "Step: 1712\tTrain acc: 0.99351\tTest acc: 0.00557\n",
      "Step: 1819\tTrain acc: 0.99444\tTest acc: 0.00557\n",
      "Step: 1926\tTrain acc: 0.99722\tTest acc: 0.00557\n",
      "Step: 2033\tTrain acc: 0.99537\tTest acc: 0.00557\n",
      "Step: 2140\tTrain acc: 0.99815\tTest acc: 0.00557\n",
      "Step: 2247\tTrain acc: 0.99815\tTest acc: 0.00557\n",
      "Step: 2354\tTrain acc: 0.99907\tTest acc: 0.00557\n",
      "Step: 2461\tTrain acc: 0.99722\tTest acc: 0.00557\n",
      "Step: 2568\tTrain acc: 0.99907\tTest acc: 0.00557\n",
      "Step: 2675\tTrain acc: 0.99815\tTest acc: 0.00557\n",
      "Step: 2782\tTrain acc: 0.99907\tTest acc: 0.00557\n",
      "Step: 2889\tTrain acc: 0.99722\tTest acc: 0.00557\n",
      "Step: 2996\tTrain acc: 0.99907\tTest acc: 0.00557\n",
      "Optimization finished!\n",
      "Training accuracy: 1.00\n",
      "Test accuracy: 0.01\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9990732159406858, 0.005571030640668524)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def load_mnist1(normalize=True, one_hot_label=False, shuffled=True):\n",
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "            \n",
    "        return T\n",
    "\n",
    "    def train_test_split(data, target, test_size, shuffled=True, seed=1004):\n",
    "        import numpy as np\n",
    "        \n",
    "        test_num = int(data.shape[0] * test_size)\n",
    "        train_num = data.shape[0] - test_num\n",
    "\n",
    "        if shuffled:\n",
    "            np.random.seed(seed)\n",
    "            shuffled = np.random.permutation(data.shape[0])\n",
    "            data = data[shuffled,:]\n",
    "            target = target[shuffled]\n",
    "        else:\n",
    "            idx = np.argsort(target)\n",
    "            data = data[idx]\n",
    "            target = target[idx]\n",
    "\n",
    "        x_train = data[:train_num]\n",
    "        x_test = data[train_num:]\n",
    "        t_train = target[:train_num]\n",
    "        t_test = target[train_num:]\n",
    "\n",
    "        return x_train, x_test, t_train, t_test\n",
    "\n",
    "    data = load_digits().data\n",
    "    target = load_digits().target\n",
    "    \n",
    "\n",
    "    x_train, x_test, t_train, t_test = train_test_split(data, target, test_size=0.4, shuffled = shuffled)\n",
    "    if normalize:\n",
    "        x_train = x_train / 16.\n",
    "        x_test = x_test / 16.\n",
    "\n",
    "    if one_hot_label:\n",
    "        t_train = _change_one_hot_label(t_train)\n",
    "        t_test = _change_one_hot_label(t_test)  \n",
    "\n",
    "    return (x_train, t_train), (x_test, t_test)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist1(shuffled=False)\n",
    "\n",
    "train_neuralnet_mnist(x_train, t_train, x_test, t_test, \n",
    "                     input_size=64, hidden_size=10, output_size=10, \n",
    "                     iters_num = 3000, batch_size = 10, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step:  108\tTrain acc: 0.18652\tTest acc: 0.17647\n",
      "Step:  216\tTrain acc: 0.29548\tTest acc: 0.30532\n",
      "Step:  324\tTrain acc: 0.82087\tTest acc: 0.78291\n",
      "Step:  432\tTrain acc: 0.87073\tTest acc: 0.82493\n",
      "Step:  540\tTrain acc: 0.90397\tTest acc: 0.85434\n",
      "Step:  648\tTrain acc: 0.93998\tTest acc: 0.88095\n",
      "Step:  756\tTrain acc: 0.95014\tTest acc: 0.89216\n",
      "Step:  864\tTrain acc: 0.95476\tTest acc: 0.89076\n",
      "Step:  972\tTrain acc: 0.96399\tTest acc: 0.89636\n",
      "Optimization finished!\n",
      "Training accuracy: 0.96\n",
      "Test accuracy: 0.90\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9630655586334257, 0.9019607843137255)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# modified from: http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def load_mnist2(normalize=True, one_hot_label=False, shuffled=True):\n",
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "            \n",
    "        return T\n",
    "\n",
    "    def train_test_split(data, target, test_size, shuffled=True, seed=1004):\n",
    "        import numpy as np\n",
    "        \n",
    "        test_num = int(data.shape[0] * test_size)\n",
    "        train_num = data.shape[0] - test_num\n",
    "\n",
    "        if shuffled:\n",
    "            np.random.seed(seed)\n",
    "            shuffled = np.random.permutation(data.shape[0])\n",
    "            data = data[shuffled,:]\n",
    "            target = target[shuffled]\n",
    "        else:\n",
    "            idx = np.argsort(target)\n",
    "            data = data[idx]\n",
    "            target = target[idx]\n",
    "\n",
    "        idx = np.where(target == 0)\n",
    "        data_split = data[idx]\n",
    "        target_split = target[idx]\n",
    "\n",
    "        test_num = int(data_split.shape[0] * test_size)\n",
    "        train_num = data_split.shape[0] - test_num\n",
    "\n",
    "        x_train = data_split[:train_num]\n",
    "        x_test = data_split[train_num:]\n",
    "        t_train = target_split[:train_num]\n",
    "        t_test = target_split[train_num:]\n",
    "\n",
    "        for i in range(1,10):\n",
    "            idx = np.where(target == i)\n",
    "            data_split = data[idx]\n",
    "            target_split = target[idx]\n",
    "\n",
    "            test_num = int(data_split.shape[0] * 0.4)\n",
    "            train_num = data_split.shape[0] - test_num\n",
    "\n",
    "            x_train = np.append(x_train, data_split[:train_num], axis=0)\n",
    "            x_test = np.append(x_test, data_split[train_num:], axis=0)\n",
    "            t_train = np.append(t_train, target_split[:train_num], axis=0)\n",
    "            t_test = np.append(t_test, target_split[train_num:], axis=0)\n",
    "        \n",
    "\n",
    "\n",
    "        # x_train = data[:train_num]\n",
    "        # x_test = data[train_num:]\n",
    "        # t_train = target[:train_num]\n",
    "        # t_test = target[train_num:]\n",
    "\n",
    "        return x_train, x_test, t_train, t_test\n",
    "\n",
    "    data = load_digits().data\n",
    "    target = load_digits().target\n",
    "    \n",
    "\n",
    "    x_train, x_test, t_train, t_test = train_test_split(data, target, test_size=0.4, shuffled = shuffled)\n",
    "    if normalize:\n",
    "        x_train = x_train / 16.\n",
    "        x_test = x_test / 16.\n",
    "\n",
    "    if one_hot_label:\n",
    "        t_train = _change_one_hot_label(t_train)\n",
    "        t_test = _change_one_hot_label(t_test)  \n",
    "\n",
    "    return (x_train, t_train), (x_test, t_test)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist2(shuffled=False)\n",
    "\n",
    "\n",
    "train_neuralnet_mnist(x_train, t_train, x_test, t_test, \n",
    "                     input_size=64, hidden_size=10, output_size=10, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step:   54\tTrain acc: 0.28624\tTest acc: 0.28992\n",
      "Step:  108\tTrain acc: 0.38412\tTest acc: 0.37395\n",
      "Step:  162\tTrain acc: 0.66667\tTest acc: 0.68627\n",
      "Step:  216\tTrain acc: 0.79224\tTest acc: 0.78711\n",
      "Step:  270\tTrain acc: 0.85411\tTest acc: 0.84034\n",
      "Step:  324\tTrain acc: 0.88181\tTest acc: 0.85854\n",
      "Step:  378\tTrain acc: 0.91043\tTest acc: 0.89776\n",
      "Step:  432\tTrain acc: 0.92336\tTest acc: 0.89916\n",
      "Step:  486\tTrain acc: 0.93352\tTest acc: 0.92017\n",
      "Step:  540\tTrain acc: 0.94552\tTest acc: 0.93277\n",
      "Step:  594\tTrain acc: 0.94829\tTest acc: 0.93137\n",
      "Step:  648\tTrain acc: 0.93813\tTest acc: 0.92017\n",
      "Step:  702\tTrain acc: 0.96030\tTest acc: 0.94818\n",
      "Step:  756\tTrain acc: 0.96307\tTest acc: 0.94678\n",
      "Step:  810\tTrain acc: 0.95753\tTest acc: 0.94118\n",
      "Step:  864\tTrain acc: 0.97045\tTest acc: 0.94818\n",
      "Step:  918\tTrain acc: 0.97230\tTest acc: 0.95098\n",
      "Step:  972\tTrain acc: 0.96676\tTest acc: 0.95378\n",
      "Step: 1026\tTrain acc: 0.97507\tTest acc: 0.96218\n",
      "Step: 1080\tTrain acc: 0.97138\tTest acc: 0.95658\n",
      "Step: 1134\tTrain acc: 0.96861\tTest acc: 0.94958\n",
      "Step: 1188\tTrain acc: 0.97322\tTest acc: 0.95378\n",
      "Step: 1242\tTrain acc: 0.97692\tTest acc: 0.95518\n",
      "Step: 1296\tTrain acc: 0.97969\tTest acc: 0.95938\n",
      "Step: 1350\tTrain acc: 0.98153\tTest acc: 0.96499\n",
      "Step: 1404\tTrain acc: 0.97692\tTest acc: 0.95518\n",
      "Step: 1458\tTrain acc: 0.97969\tTest acc: 0.96218\n",
      "Step: 1512\tTrain acc: 0.98338\tTest acc: 0.96639\n",
      "Step: 1566\tTrain acc: 0.98338\tTest acc: 0.96499\n",
      "Step: 1620\tTrain acc: 0.98430\tTest acc: 0.96639\n",
      "Step: 1674\tTrain acc: 0.98615\tTest acc: 0.96639\n",
      "Step: 1728\tTrain acc: 0.98523\tTest acc: 0.96359\n",
      "Step: 1782\tTrain acc: 0.98615\tTest acc: 0.96919\n",
      "Step: 1836\tTrain acc: 0.97692\tTest acc: 0.96499\n",
      "Step: 1890\tTrain acc: 0.98523\tTest acc: 0.97199\n",
      "Step: 1944\tTrain acc: 0.98984\tTest acc: 0.97199\n",
      "Step: 1998\tTrain acc: 0.98984\tTest acc: 0.97059\n",
      "Step: 2052\tTrain acc: 0.98707\tTest acc: 0.96919\n",
      "Step: 2106\tTrain acc: 0.98800\tTest acc: 0.96919\n",
      "Step: 2160\tTrain acc: 0.98984\tTest acc: 0.96779\n",
      "Step: 2214\tTrain acc: 0.99169\tTest acc: 0.96639\n",
      "Step: 2268\tTrain acc: 0.98984\tTest acc: 0.96779\n",
      "Step: 2322\tTrain acc: 0.98892\tTest acc: 0.96639\n",
      "Step: 2376\tTrain acc: 0.99169\tTest acc: 0.96779\n",
      "Step: 2430\tTrain acc: 0.99169\tTest acc: 0.96779\n",
      "Step: 2484\tTrain acc: 0.99261\tTest acc: 0.96499\n",
      "Step: 2538\tTrain acc: 0.99169\tTest acc: 0.96779\n",
      "Step: 2592\tTrain acc: 0.99354\tTest acc: 0.97059\n",
      "Step: 2646\tTrain acc: 0.99261\tTest acc: 0.96499\n",
      "Step: 2700\tTrain acc: 0.99538\tTest acc: 0.96919\n",
      "Step: 2754\tTrain acc: 0.98892\tTest acc: 0.96779\n",
      "Step: 2808\tTrain acc: 0.98800\tTest acc: 0.96359\n",
      "Step: 2862\tTrain acc: 0.98892\tTest acc: 0.96919\n",
      "Step: 2916\tTrain acc: 0.99446\tTest acc: 0.96919\n",
      "Step: 2970\tTrain acc: 0.98430\tTest acc: 0.96639\n",
      "Step: 3024\tTrain acc: 0.99446\tTest acc: 0.96919\n",
      "Step: 3078\tTrain acc: 0.99354\tTest acc: 0.97059\n",
      "Step: 3132\tTrain acc: 0.99446\tTest acc: 0.96639\n",
      "Step: 3186\tTrain acc: 0.99631\tTest acc: 0.97199\n",
      "Step: 3240\tTrain acc: 0.99538\tTest acc: 0.97059\n",
      "Step: 3294\tTrain acc: 0.99631\tTest acc: 0.96499\n",
      "Step: 3348\tTrain acc: 0.99631\tTest acc: 0.96779\n",
      "Step: 3402\tTrain acc: 0.99631\tTest acc: 0.97059\n",
      "Step: 3456\tTrain acc: 0.99446\tTest acc: 0.96779\n",
      "Step: 3510\tTrain acc: 0.99538\tTest acc: 0.97059\n",
      "Step: 3564\tTrain acc: 0.99723\tTest acc: 0.97059\n",
      "Step: 3618\tTrain acc: 0.99538\tTest acc: 0.96779\n",
      "Step: 3672\tTrain acc: 0.99631\tTest acc: 0.97199\n",
      "Step: 3726\tTrain acc: 0.99723\tTest acc: 0.96919\n",
      "Step: 3780\tTrain acc: 0.99446\tTest acc: 0.96779\n",
      "Step: 3834\tTrain acc: 0.99538\tTest acc: 0.97339\n",
      "Step: 3888\tTrain acc: 0.99723\tTest acc: 0.96779\n",
      "Step: 3942\tTrain acc: 0.99631\tTest acc: 0.97059\n",
      "Step: 3996\tTrain acc: 0.99723\tTest acc: 0.97339\n",
      "Step: 4050\tTrain acc: 0.99631\tTest acc: 0.96779\n",
      "Step: 4104\tTrain acc: 0.99446\tTest acc: 0.97059\n",
      "Step: 4158\tTrain acc: 0.99723\tTest acc: 0.97339\n",
      "Step: 4212\tTrain acc: 0.99723\tTest acc: 0.96779\n",
      "Step: 4266\tTrain acc: 0.99723\tTest acc: 0.96779\n",
      "Step: 4320\tTrain acc: 0.99631\tTest acc: 0.96639\n",
      "Step: 4374\tTrain acc: 0.99446\tTest acc: 0.96779\n",
      "Step: 4428\tTrain acc: 0.99631\tTest acc: 0.97479\n",
      "Step: 4482\tTrain acc: 0.99815\tTest acc: 0.96919\n",
      "Step: 4536\tTrain acc: 0.99815\tTest acc: 0.97339\n",
      "Step: 4590\tTrain acc: 0.99723\tTest acc: 0.96639\n",
      "Step: 4644\tTrain acc: 0.99815\tTest acc: 0.97199\n",
      "Step: 4698\tTrain acc: 0.99815\tTest acc: 0.96779\n",
      "Step: 4752\tTrain acc: 0.99723\tTest acc: 0.96499\n",
      "Step: 4806\tTrain acc: 0.99815\tTest acc: 0.96779\n",
      "Step: 4860\tTrain acc: 0.99815\tTest acc: 0.96639\n",
      "Step: 4914\tTrain acc: 0.99815\tTest acc: 0.96639\n",
      "Step: 4968\tTrain acc: 0.99815\tTest acc: 0.96499\n",
      "Step: 5022\tTrain acc: 0.99815\tTest acc: 0.96639\n",
      "Step: 5076\tTrain acc: 0.99815\tTest acc: 0.96919\n",
      "Step: 5130\tTrain acc: 0.99815\tTest acc: 0.97199\n",
      "Step: 5184\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 5238\tTrain acc: 0.99908\tTest acc: 0.97339\n",
      "Step: 5292\tTrain acc: 0.99815\tTest acc: 0.96779\n",
      "Step: 5346\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 5400\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 5454\tTrain acc: 0.99908\tTest acc: 0.97339\n",
      "Step: 5508\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 5562\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 5616\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 5670\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 5724\tTrain acc: 0.99908\tTest acc: 0.97199\n",
      "Step: 5778\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 5832\tTrain acc: 0.99723\tTest acc: 0.96639\n",
      "Step: 5886\tTrain acc: 0.99815\tTest acc: 0.96919\n",
      "Step: 5940\tTrain acc: 0.99815\tTest acc: 0.97059\n",
      "Step: 5994\tTrain acc: 0.99815\tTest acc: 0.96779\n",
      "Step: 6048\tTrain acc: 0.99815\tTest acc: 0.96639\n",
      "Step: 6102\tTrain acc: 0.99815\tTest acc: 0.96639\n",
      "Step: 6156\tTrain acc: 0.99815\tTest acc: 0.96919\n",
      "Step: 6210\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 6264\tTrain acc: 0.99908\tTest acc: 0.97199\n",
      "Step: 6318\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 6372\tTrain acc: 0.99908\tTest acc: 0.97339\n",
      "Step: 6426\tTrain acc: 0.99908\tTest acc: 0.96499\n",
      "Step: 6480\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 6534\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 6588\tTrain acc: 0.99908\tTest acc: 0.97339\n",
      "Step: 6642\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 6696\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 6750\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 6804\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 6858\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 6912\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 6966\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 7020\tTrain acc: 0.99815\tTest acc: 0.96639\n",
      "Step: 7074\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 7128\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 7182\tTrain acc: 0.99908\tTest acc: 0.97199\n",
      "Step: 7236\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 7290\tTrain acc: 1.00000\tTest acc: 0.96499\n",
      "Step: 7344\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 7398\tTrain acc: 0.99815\tTest acc: 0.97059\n",
      "Step: 7452\tTrain acc: 0.99908\tTest acc: 0.97199\n",
      "Step: 7506\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 7560\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 7614\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 7668\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 7722\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 7776\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 7830\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 7884\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 7938\tTrain acc: 1.00000\tTest acc: 0.96639\n",
      "Step: 7992\tTrain acc: 0.99908\tTest acc: 0.97199\n",
      "Step: 8046\tTrain acc: 0.99908\tTest acc: 0.96639\n",
      "Step: 8100\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 8154\tTrain acc: 0.99908\tTest acc: 0.97059\n",
      "Step: 8208\tTrain acc: 1.00000\tTest acc: 0.96499\n",
      "Step: 8262\tTrain acc: 1.00000\tTest acc: 0.96639\n",
      "Step: 8316\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 8370\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 8424\tTrain acc: 0.99908\tTest acc: 0.96499\n",
      "Step: 8478\tTrain acc: 1.00000\tTest acc: 0.97199\n",
      "Step: 8532\tTrain acc: 1.00000\tTest acc: 0.97059\n",
      "Step: 8586\tTrain acc: 1.00000\tTest acc: 0.96639\n",
      "Step: 8640\tTrain acc: 1.00000\tTest acc: 0.96499\n",
      "Step: 8694\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 8748\tTrain acc: 1.00000\tTest acc: 0.97059\n",
      "Step: 8802\tTrain acc: 0.99908\tTest acc: 0.96499\n",
      "Step: 8856\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 8910\tTrain acc: 1.00000\tTest acc: 0.97059\n",
      "Step: 8964\tTrain acc: 1.00000\tTest acc: 0.96499\n",
      "Step: 9018\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 9072\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 9126\tTrain acc: 1.00000\tTest acc: 0.96639\n",
      "Step: 9180\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 9234\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 9288\tTrain acc: 1.00000\tTest acc: 0.97059\n",
      "Step: 9342\tTrain acc: 1.00000\tTest acc: 0.96499\n",
      "Step: 9396\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 9450\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 9504\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 9558\tTrain acc: 1.00000\tTest acc: 0.96639\n",
      "Step: 9612\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 9666\tTrain acc: 0.99908\tTest acc: 0.97199\n",
      "Step: 9720\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Step: 9774\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 9828\tTrain acc: 0.99908\tTest acc: 0.96779\n",
      "Step: 9882\tTrain acc: 0.99908\tTest acc: 0.96919\n",
      "Step: 9936\tTrain acc: 1.00000\tTest acc: 0.96919\n",
      "Step: 9990\tTrain acc: 1.00000\tTest acc: 0.96779\n",
      "Optimization finished!\n",
      "Training accuracy: 1.00\n",
      "Test accuracy: 0.96\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n# training and testing\\ngnb = GaussianNB().fit(iris.data[Itr,:], iris.target[Itr])\\ny_pred = gnb.predict(iris.data[Ite,:])\\nnmisses = (iris.target[Ite] != y_pred).sum()\\nprint('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\\n        % (sum(Ite), nmisses, float(nmisses)/sum(Ite)*100.0))\\n\\n# [Multiple Executions]\\n# Number of mislabeled out of a total 60 samples : 3 (5.00%)\\n# Number of mislabeled out of a total 60 samples : 4 (6.67%)\\n# Number of mislabeled out of a total 60 samples : 0 (0.00%)\\n# Number of mislabeled out of a total 60 samples : 5 (8.33%)\\n# Number of mislabeled out of a total 60 samples : 1 (1.67%)\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def load_mnist3(normalize=True, one_hot_label=False, shuffled=True):\n",
    "    def _change_one_hot_label(X):\n",
    "        T = np.zeros((X.size, 10))\n",
    "        for idx, row in enumerate(T):\n",
    "            row[X[idx]] = 1\n",
    "            \n",
    "        return T\n",
    "\n",
    "    def train_test_split(data, target, test_size, shuffled=True, seed=1004):\n",
    "        import numpy as np\n",
    "        \n",
    "        test_num = int(data.shape[0] * test_size)\n",
    "        train_num = data.shape[0] - test_num\n",
    "\n",
    "        if shuffled:\n",
    "            np.random.seed(seed)\n",
    "            shuffled = np.random.permutation(data.shape[0])\n",
    "            data = data[shuffled,:]\n",
    "            target = target[shuffled]\n",
    "        else:\n",
    "            idx = np.argsort(target)\n",
    "            data = data[idx]\n",
    "            target = target[idx]\n",
    "\n",
    "        idx = np.where(target == 0)\n",
    "        data_split = data[idx]\n",
    "        target_split = target[idx]\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        shuffled = np.random.permutation(data_split.shape[0])\n",
    "        data_split = data_split[shuffled,:]\n",
    "        target_split = target_split[shuffled]\n",
    "\n",
    "        test_num = int(data_split.shape[0] * test_size)\n",
    "        train_num = data_split.shape[0] - test_num\n",
    "\n",
    "        x_train = data_split[:train_num]\n",
    "        x_test = data_split[train_num:]\n",
    "        t_train = target_split[:train_num]\n",
    "        t_test = target_split[train_num:]\n",
    "\n",
    "        for i in range(1,10):\n",
    "            idx = np.where(target == i)\n",
    "            data_split = data[idx]\n",
    "            target_split = target[idx]\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            shuffled = np.random.permutation(data_split.shape[0])\n",
    "            data_split = data_split[shuffled,:]\n",
    "            target_split = target_split[shuffled]\n",
    "\n",
    "            test_num = int(data_split.shape[0] * 0.4)\n",
    "            train_num = data_split.shape[0] - test_num\n",
    "\n",
    "            x_train = np.append(x_train, data_split[:train_num], axis=0)\n",
    "            x_test = np.append(x_test, data_split[train_num:], axis=0)\n",
    "            t_train = np.append(t_train, target_split[:train_num], axis=0)\n",
    "            t_test = np.append(t_test, target_split[train_num:], axis=0)\n",
    "    \n",
    "\n",
    "\n",
    "        # x_train = data[:train_num]\n",
    "        # x_test = data[train_num:]\n",
    "        # t_train = target[:train_num]\n",
    "        # t_test = target[train_num:]\n",
    "\n",
    "        return x_train, x_test, t_train, t_test\n",
    "\n",
    "    data = load_digits().data\n",
    "    target = load_digits().target\n",
    "    \n",
    "\n",
    "    x_train, x_test, t_train, t_test = train_test_split(data, target, test_size=0.4, shuffled = shuffled)\n",
    "    if normalize:\n",
    "        x_train = x_train / 16.\n",
    "        x_test = x_test / 16.\n",
    "\n",
    "    if one_hot_label:\n",
    "        t_train = _change_one_hot_label(t_train)\n",
    "        t_test = _change_one_hot_label(t_test)  \n",
    "\n",
    "    return (x_train, t_train), (x_test, t_test)\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist3(shuffled=False)\n",
    "\n",
    "\n",
    "train_neuralnet_mnist(x_train, t_train, x_test, t_test, \n",
    "                     input_size=64, hidden_size=100, output_size=10, \n",
    "                     iters_num = 10000, batch_size = 20, learning_rate = 0.1)\n",
    "\n",
    "\n",
    "'''\n",
    "# training and testing\n",
    "gnb = GaussianNB().fit(iris.data[Itr,:], iris.target[Itr])\n",
    "y_pred = gnb.predict(iris.data[Ite,:])\n",
    "nmisses = (iris.target[Ite] != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (sum(Ite), nmisses, float(nmisses)/sum(Ite)*100.0))\n",
    "\n",
    "# [Multiple Executions]\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 4 (6.67%)\n",
    "# Number of mislabeled out of a total 60 samples : 0 (0.00%)\n",
    "# Number of mislabeled out of a total 60 samples : 5 (8.33%)\n",
    "# Number of mislabeled out of a total 60 samples : 1 (1.67%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step:  107\tTrain acc: 0.10111\tTest acc: 0.09736\n",
      "Step:  214\tTrain acc: 0.25788\tTest acc: 0.23088\n",
      "Step:  321\tTrain acc: 0.65306\tTest acc: 0.62587\n",
      "Step:  428\tTrain acc: 0.77087\tTest acc: 0.75939\n",
      "Step:  535\tTrain acc: 0.81911\tTest acc: 0.82058\n",
      "Step:  642\tTrain acc: 0.91002\tTest acc: 0.91099\n",
      "Step:  749\tTrain acc: 0.92301\tTest acc: 0.91655\n",
      "Step:  856\tTrain acc: 0.92950\tTest acc: 0.93046\n",
      "Step:  963\tTrain acc: 0.94805\tTest acc: 0.94159\n",
      "Step: 1070\tTrain acc: 0.95083\tTest acc: 0.94019\n",
      "Step: 1177\tTrain acc: 0.95547\tTest acc: 0.94576\n",
      "Step: 1284\tTrain acc: 0.95547\tTest acc: 0.93880\n",
      "Step: 1391\tTrain acc: 0.96568\tTest acc: 0.95132\n",
      "Step: 1498\tTrain acc: 0.96568\tTest acc: 0.94854\n",
      "Step: 1605\tTrain acc: 0.95826\tTest acc: 0.93185\n",
      "Step: 1712\tTrain acc: 0.97495\tTest acc: 0.94854\n",
      "Step: 1819\tTrain acc: 0.98052\tTest acc: 0.95410\n",
      "Step: 1926\tTrain acc: 0.97774\tTest acc: 0.95688\n",
      "Step: 2033\tTrain acc: 0.97124\tTest acc: 0.95410\n",
      "Step: 2140\tTrain acc: 0.98237\tTest acc: 0.95828\n",
      "Step: 2247\tTrain acc: 0.98516\tTest acc: 0.96106\n",
      "Step: 2354\tTrain acc: 0.98145\tTest acc: 0.94159\n",
      "Step: 2461\tTrain acc: 0.98237\tTest acc: 0.95410\n",
      "Step: 2568\tTrain acc: 0.98609\tTest acc: 0.95688\n",
      "Step: 2675\tTrain acc: 0.98609\tTest acc: 0.95828\n",
      "Step: 2782\tTrain acc: 0.99072\tTest acc: 0.95688\n",
      "Step: 2889\tTrain acc: 0.99165\tTest acc: 0.95828\n",
      "Step: 2996\tTrain acc: 0.98794\tTest acc: 0.95549\n",
      "Optimization finished!\n",
      "Training accuracy: 0.98\n",
      "Test accuracy: 0.95\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9833024118738405, 0.9513212795549374)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits().data\n",
    "target = load_digits().target\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(data, target, test_size=0.4, shuffle=True)\n",
    "\n",
    "x_train = x_train / 16.\n",
    "x_test = x_test / 16.\n",
    "\n",
    "train_neuralnet_mnist(x_train, t_train, x_test, t_test, \n",
    "                     input_size=64, hidden_size=10, output_size=10, \n",
    "                     iters_num = 3000, batch_size = 20, learning_rate = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step:   53\tTrain acc: 0.10111\tTest acc: 0.10153\n",
      "Step:  106\tTrain acc: 0.18275\tTest acc: 0.17942\n",
      "Step:  159\tTrain acc: 0.47032\tTest acc: 0.47288\n",
      "Step:  212\tTrain acc: 0.56865\tTest acc: 0.56885\n",
      "Step:  265\tTrain acc: 0.79685\tTest acc: 0.80389\n",
      "Step:  318\tTrain acc: 0.82931\tTest acc: 0.85257\n",
      "Step:  371\tTrain acc: 0.86364\tTest acc: 0.88595\n",
      "Step:  424\tTrain acc: 0.89703\tTest acc: 0.91238\n",
      "Step:  477\tTrain acc: 0.91558\tTest acc: 0.90682\n",
      "Step:  530\tTrain acc: 0.92115\tTest acc: 0.92350\n",
      "Step:  583\tTrain acc: 0.93599\tTest acc: 0.93324\n",
      "Step:  636\tTrain acc: 0.91558\tTest acc: 0.91933\n",
      "Step:  689\tTrain acc: 0.95269\tTest acc: 0.94576\n",
      "Step:  742\tTrain acc: 0.95640\tTest acc: 0.94576\n",
      "Step:  795\tTrain acc: 0.95640\tTest acc: 0.94576\n",
      "Step:  848\tTrain acc: 0.96475\tTest acc: 0.94576\n",
      "Step:  901\tTrain acc: 0.96475\tTest acc: 0.94576\n",
      "Step:  954\tTrain acc: 0.95918\tTest acc: 0.95410\n",
      "Step: 1007\tTrain acc: 0.95918\tTest acc: 0.93880\n",
      "Step: 1060\tTrain acc: 0.96846\tTest acc: 0.95549\n",
      "Step: 1113\tTrain acc: 0.97124\tTest acc: 0.95688\n",
      "Step: 1166\tTrain acc: 0.97681\tTest acc: 0.95549\n",
      "Step: 1219\tTrain acc: 0.97217\tTest acc: 0.95828\n",
      "Step: 1272\tTrain acc: 0.97774\tTest acc: 0.95967\n",
      "Step: 1325\tTrain acc: 0.97959\tTest acc: 0.95549\n",
      "Step: 1378\tTrain acc: 0.97774\tTest acc: 0.95549\n",
      "Step: 1431\tTrain acc: 0.98052\tTest acc: 0.96106\n",
      "Step: 1484\tTrain acc: 0.98237\tTest acc: 0.96106\n",
      "Step: 1537\tTrain acc: 0.98423\tTest acc: 0.96523\n",
      "Step: 1590\tTrain acc: 0.98423\tTest acc: 0.95549\n",
      "Step: 1643\tTrain acc: 0.98330\tTest acc: 0.95549\n",
      "Step: 1696\tTrain acc: 0.98423\tTest acc: 0.95688\n",
      "Step: 1749\tTrain acc: 0.98330\tTest acc: 0.95688\n",
      "Step: 1802\tTrain acc: 0.98423\tTest acc: 0.95967\n",
      "Step: 1855\tTrain acc: 0.98609\tTest acc: 0.96245\n",
      "Step: 1908\tTrain acc: 0.98794\tTest acc: 0.96523\n",
      "Step: 1961\tTrain acc: 0.99072\tTest acc: 0.95967\n",
      "Step: 2014\tTrain acc: 0.98794\tTest acc: 0.96662\n",
      "Step: 2067\tTrain acc: 0.98887\tTest acc: 0.95967\n",
      "Step: 2120\tTrain acc: 0.98794\tTest acc: 0.95549\n",
      "Step: 2173\tTrain acc: 0.99165\tTest acc: 0.95967\n",
      "Step: 2226\tTrain acc: 0.99072\tTest acc: 0.95828\n",
      "Step: 2279\tTrain acc: 0.99165\tTest acc: 0.96384\n",
      "Step: 2332\tTrain acc: 0.98794\tTest acc: 0.96245\n",
      "Step: 2385\tTrain acc: 0.99258\tTest acc: 0.96245\n",
      "Step: 2438\tTrain acc: 0.99351\tTest acc: 0.95967\n",
      "Step: 2491\tTrain acc: 0.98980\tTest acc: 0.96106\n",
      "Step: 2544\tTrain acc: 0.99165\tTest acc: 0.95828\n",
      "Step: 2597\tTrain acc: 0.99258\tTest acc: 0.95828\n",
      "Step: 2650\tTrain acc: 0.98980\tTest acc: 0.95828\n",
      "Step: 2703\tTrain acc: 0.99165\tTest acc: 0.95828\n",
      "Step: 2756\tTrain acc: 0.99351\tTest acc: 0.95688\n",
      "Step: 2809\tTrain acc: 0.99258\tTest acc: 0.96106\n",
      "Step: 2862\tTrain acc: 0.99443\tTest acc: 0.96523\n",
      "Step: 2915\tTrain acc: 0.98794\tTest acc: 0.95967\n",
      "Step: 2968\tTrain acc: 0.99536\tTest acc: 0.95688\n",
      "Step: 3021\tTrain acc: 0.99629\tTest acc: 0.96384\n",
      "Step: 3074\tTrain acc: 0.99443\tTest acc: 0.96245\n",
      "Step: 3127\tTrain acc: 0.99443\tTest acc: 0.96106\n",
      "Step: 3180\tTrain acc: 0.99629\tTest acc: 0.95967\n",
      "Step: 3233\tTrain acc: 0.99536\tTest acc: 0.95967\n",
      "Step: 3286\tTrain acc: 0.99722\tTest acc: 0.95967\n",
      "Step: 3339\tTrain acc: 0.99351\tTest acc: 0.96245\n",
      "Step: 3392\tTrain acc: 0.99536\tTest acc: 0.96245\n",
      "Step: 3445\tTrain acc: 0.99722\tTest acc: 0.95828\n",
      "Step: 3498\tTrain acc: 0.99722\tTest acc: 0.96106\n",
      "Step: 3551\tTrain acc: 0.99536\tTest acc: 0.95967\n",
      "Step: 3604\tTrain acc: 0.99722\tTest acc: 0.95688\n",
      "Step: 3657\tTrain acc: 0.99629\tTest acc: 0.95967\n",
      "Step: 3710\tTrain acc: 0.99814\tTest acc: 0.96106\n",
      "Step: 3763\tTrain acc: 0.99722\tTest acc: 0.95828\n",
      "Step: 3816\tTrain acc: 0.99629\tTest acc: 0.96106\n",
      "Step: 3869\tTrain acc: 0.99722\tTest acc: 0.96384\n",
      "Step: 3922\tTrain acc: 0.99814\tTest acc: 0.95688\n",
      "Step: 3975\tTrain acc: 0.99814\tTest acc: 0.96384\n",
      "Optimization finished!\n",
      "Training accuracy: 1.00\n",
      "Test accuracy: 0.96\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9972170686456401, 0.9638386648122392)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# HO4: Reproducible Random Sampling\n",
    "# Random sampling by sklearn.model_selection.train_test_split\n",
    "# source: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits().data\n",
    "target = load_digits().target\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(data, target, test_size=0.4, shuffle=True, random_state=len(target))\n",
    "\n",
    "np.random.seed(len(target))\n",
    "\n",
    "x_train = x_train / 16.\n",
    "x_test = x_test / 16.\n",
    "\n",
    "# fix the SEED of random permutation to be the number of samples, \n",
    "# to reproduce the same random sequence at every execution\n",
    "np.random.seed(len(target))\n",
    "\n",
    "train_neuralnet_mnist(x_train, t_train, x_test, t_test,\n",
    "                     input_size=64, hidden_size=30, output_size=10, \n",
    "                     iters_num = 4000, batch_size = 20, learning_rate = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test: 71 73 71 73 72 73 72 72 70 72,  training: 107 109 106 110 109 109 109 107 104 108,  \n",
      "Step:  107\tTrain acc: 0.17625\tTest acc: 0.18637\n",
      "Step:  214\tTrain acc: 0.30891\tTest acc: 0.30459\n",
      "Step:  321\tTrain acc: 0.75417\tTest acc: 0.74548\n",
      "Step:  428\tTrain acc: 0.84787\tTest acc: 0.82058\n",
      "Step:  535\tTrain acc: 0.91187\tTest acc: 0.88039\n",
      "Step:  642\tTrain acc: 0.91280\tTest acc: 0.88317\n",
      "Step:  749\tTrain acc: 0.93970\tTest acc: 0.91794\n",
      "Step:  856\tTrain acc: 0.93414\tTest acc: 0.91933\n",
      "Step:  963\tTrain acc: 0.95362\tTest acc: 0.93185\n",
      "Optimization finished!\n",
      "Training accuracy: 0.96\n",
      "Test accuracy: 0.93\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"# training and testing \\ny_pred = GaussianNB().fit(Xtr, ytr).predict(Xte)\\nnmisses = (yte != y_pred).sum()\\nprint('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\\n        % (len(yte), nmisses, float(nmisses)/len(yte)*100.0))\\n\\n# [Multiple Executions]\\n# test: 20 20 20,  training: 30 30 30\\n# Number of mislabeled out of a total 60 samples : 6 (10.00%)\\n# test: 20 20 20,  training: 30 30 30\\n# Number of mislabeled out of a total 60 samples : 3 (5.00%)\\n# test: 20 20 20,  training: 30 30 30\\n# Number of mislabeled out of a total 60 samples : 4 (6.67%)\\n# test: 20 20 20,  training: 30 30 30\\n# Number of mislabeled out of a total 60 samples : 3 (5.00%)\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# HO5: Stratified Random Sampling}\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = load_digits().data\n",
    "y = load_digits().target\n",
    "X = X / 16.\n",
    "\n",
    "# per-class random sampling by passing y to variable stratify, \n",
    "Xtr,Xte,ytr,yte = train_test_split(X, y, test_size=0.4, shuffle=True, stratify=y)\n",
    "\n",
    "# check number of samples of the individual classes\n",
    "print('test: %d %d %d %d %d %d %d %d %d %d,  '%(sum(yte==0),sum(yte==1),sum(yte==2),sum(yte==3),sum(yte==4),sum(yte==5),sum(yte==6),sum(yte==7),sum(yte==8),sum(yte==9)),end='')\n",
    "print('training: %d %d %d %d %d %d %d %d %d %d,  '%(sum(ytr==0),sum(ytr==1),sum(ytr==2),sum(ytr==3),sum(ytr==4),sum(ytr==5),sum(ytr==6),sum(ytr==7),sum(ytr==8),sum(ytr==9)))\n",
    "\n",
    "# due to the random initialization of the weights, the performance varies\n",
    "# so we have to set the random seed for TwoLayerNet's initialization values\n",
    "np.random.seed(len(y))\n",
    "\n",
    "train_neuralnet_mnist(Xtr,ytr,Xte,yte,\n",
    "                     input_size=64, hidden_size=10, output_size=10, \n",
    "                     iters_num = 1000, batch_size = 10, learning_rate = 0.1)\n",
    "\n",
    "\"\"\"# training and testing \n",
    "y_pred = GaussianNB().fit(Xtr, ytr).predict(Xte)\n",
    "nmisses = (yte != y_pred).sum()\n",
    "print('Number of mislabeled out of a total %d samples : %d (%.2f%%)'\n",
    "        % (len(yte), nmisses, float(nmisses)/len(yte)*100.0))\n",
    "\n",
    "# [Multiple Executions]\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 6 (10.00%)\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 4 (6.67%)\n",
    "# test: 20 20 20,  training: 30 30 30\n",
    "# Number of mislabeled out of a total 60 samples : 3 (5.00%)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trial 0: accuracy 0.999 0.961\n",
      "Trial 1: accuracy 1.000 0.958\n",
      "Trial 2: accuracy 0.998 0.955\n",
      "Trial 3: accuracy 1.000 0.967\n",
      "Trial 4: accuracy 0.997 0.965\n",
      "Trial 5: accuracy 0.999 0.971\n",
      "Trial 6: accuracy 0.998 0.971\n",
      "Trial 7: accuracy 0.999 0.954\n",
      "Trial 8: accuracy 0.998 0.957\n",
      "Trial 9: accuracy 0.998 0.967\n",
      "Trial 10: accuracy 1.000 0.967\n",
      "Trial 11: accuracy 1.000 0.960\n",
      "Trial 12: accuracy 1.000 0.961\n",
      "Trial 13: accuracy 0.996 0.964\n",
      "Trial 14: accuracy 0.999 0.962\n",
      "Trial 15: accuracy 0.998 0.958\n",
      "Trial 16: accuracy 0.997 0.971\n",
      "Trial 17: accuracy 0.998 0.975\n",
      "Trial 18: accuracy 0.999 0.960\n",
      "Trial 19: accuracy 0.998 0.960\n"
     ]
    }
   ],
   "source": [
    "# Repeated Random Subsampling\n",
    "# Repeating stratified random sampling K times\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = load_digits().data\n",
    "y = load_digits().target\n",
    "X = X / 16.\n",
    "\n",
    "# due to the random initialization of the weights, the performance varies\n",
    "# so we have to set the random seed for TwoLayerNet's initialization values\n",
    "np.random.seed(len(y))\n",
    "\n",
    "K = 20\n",
    "Acc = np.zeros([K,2], dtype=float)\n",
    "for k in range(K):\n",
    "    # stratified random sampling\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.4, shuffle=True, random_state=None, stratify=y)\n",
    "    Acc[k,0], Acc[k,1] = train_neuralnet_mnist(Xtr,ytr,Xte,yte,\n",
    "                                  input_size=64, hidden_size=20, output_size=10, \n",
    "                                  iters_num = 10000, batch_size = 20, learning_rate = 0.05, \n",
    "                                  verbose = False)\n",
    "    print('Trial %d: accuracy %.3f %.3f' % (k, Acc[k,0], Acc[k,1]))\n",
    "\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.6 (4.42%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.8 (4.67%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.6 (4.33%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.8 (4.58%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 3.0 (5.08%)\n",
    "# 20 trials, average mislabeled out of a total 60 samples : 2.7 (4.50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}